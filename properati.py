# -*- coding: utf-8 -*-
"""Properati.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PmOlIHdvZqplLoeTorGWKdS88zAXC7WH

#Importar librerias
"""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# shred -u setup_colab_general.py
# wget -q "https://github.com/jpcano1/python_utils/raw/main/setup_colab_general.py" -O setup_colab_general.py
# pip install -q https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

from pandas_profiling import ProfileReport

"""#Importación de datos"""

import pandas as pd

df = pd.read_csv ('/content/co_properties_20220315_t1.txt', delimiter=';',  decimal=",")

df.head(3)

ProfileReport(df)

df = df.drop(['end_date','start_date','l1',' id','l5','l6', 'ad_type', 'currency'], axis=1)

df_ventas = df.query("property_type == ['Casa','Apartamento'] & operation_type =='Venta'")

ProfileReport(df_ventas[~df_ventas[['property_type', 'created_on', 'lat', 'lon', 'price']].isnull().any(axis=1)])

df['surface'] = df['surface_total'].combine_first(df['surface_covered'])
df['rooms_bedrooms'] = df['rooms'].combine_first(df['bedrooms'])

"""#Filtro de datos"""

df_ventas = df.query("property_type == ['Casa','Apartamento'] & operation_type =='Venta' & lat < 13.34 & lat > -4.2 & lon < -67.49 & lon > -81.36 ")

print(' Apartamento -------------------')

print('0.5% of data')
q1=df_ventas[df_ventas['property_type']=='Apartamento'].quantile(0.005)
print(q1)
q3=df_ventas[df_ventas['property_type']=='Apartamento'].quantile(0.995)

print('\n 99.5% of data')
print(q3)


print(' Casa -------------------')

print('0.5% of data')
q1=df_ventas[df_ventas['property_type']=='Casa'].quantile(0.005)
print(q1)
q3=df_ventas[df_ventas['property_type']=='Casa'].quantile(0.995)

print('\n 99.5% of data')
print(q3)

df_ventas = df.query("property_type == ['Casa','Apartamento'] & operation_type =='Venta' & price > 5.5e7 & price < 5e9 & lat < 13.34 & lat > -4.2 & lon < -67.49 & lon > -81.36 & rooms_bedrooms>0 & rooms_bedrooms<15 & l2 !=['Florida']")

df_ventas = df_ventas.drop(['rooms','bedrooms', 'surface_total','surface_covered'], axis=1)

df_ventas['created_on'] =  pd.to_datetime(df_ventas['created_on'],  format='%Y-%m-%d', errors='coerce')

df_ventas['month'] = df_ventas['created_on'].dt.month
df_ventas['year'] = df_ventas['created_on'].dt.year

ProfileReport(df_ventas[~df_ventas[['property_type', 'created_on', 'lat', 'lon', 'price']].isnull().any(axis=1)])

df_ventas = df_ventas[~df_ventas[['property_type', 'operation_type', 'created_on', 'lat', 'lon', 'price', ]].isnull().any(axis=1)]

df_ventas.to_csv('df_ventas.csv')

from sklearn.model_selection import train_test_split
import numpy as np

X_train, X_test, y_train, y_test = train_test_split(df_ventas[['property_type', 'lat', 'lon', 'bathrooms', 
                                                               'rooms_bedrooms']], np.log1p(df_ventas['price']), test_size=0.2,  random_state=1)

"""#Tratamiento de datos"""

from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MinMaxScaler, MaxAbsScaler, LabelEncoder, LabelBinarizer
from sklearn.impute import KNNImputer
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn_pandas import DataFrameMapper
from sklearn.compose import make_column_selector, ColumnTransformer
import numpy as np
from sklearn.compose import make_column_transformer


ct = ColumnTransformer([('cat',
       OneHotEncoder(),
       make_column_selector(dtype_include=object)),
       ('num', KNNImputer(n_neighbors= 3),
       make_column_selector(dtype_include=np.number))
        ])


estimadores = [('encoder_imputer', ct)]

pipe = Pipeline(estimadores)

X_trainTrans = pipe.fit_transform(X_train)

X_testTrans = pipe.transform(X_test)

"""# K Neighbor regressor"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score, TimeSeriesSplit, RandomizedSearchCV

KNN_algorithm = KNeighborsRegressor()
prams={'n_neighbors': [4,8,16], 
       'weights': ['uniform','distance']}


ksearch = GridSearchCV(estimator=KNN_algorithm, cv=5, param_grid=prams, scoring = 'neg_root_mean_squared_error',   return_train_score=True)
ksearch.fit(X_trainTrans, y_train)

ksearch.best_estimator_

resultadosKNN = pd.DataFrame(ksearch.cv_results_)
display(resultadosKNN)

"""Nota: Por qué los valores son negativos: https://stackoverflow.com/questions/21443865/scikit-learn-cross-validation-negative-values-with-mean-squared-error 

https://scikit-learn.org/stable/modules/model_evaluation.html

# Random forest regressor
"""

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(random_state=1234)
param_search = { 
    'n_estimators': [50,100,200],
    'max_depth':[20,50,100],
}

gsearch = GridSearchCV(estimator=model, cv=5, param_grid=param_search, scoring = 'neg_root_mean_squared_error',   return_train_score=True)
gsearch.fit(X_trainTrans, y_train)


gsearch.best_estimator_

resultadosRF = pd.DataFrame(gsearch.cv_results_)
display(resultadosRF)

import matplotlib.pyplot as plt
import seaborn as sns
importances = pd.DataFrame({'Importancias':gsearch.best_estimator_.feature_importances_,
                            'Caracteristicas':pd.get_dummies(X_train).columns})
importances = importances.sort_values("Importancias", ascending=False)
plt.figure(figsize=(15,8))
sns.set_color_codes("muted")

sns.barplot(x="Importancias", y="Caracteristicas", data=importances, 
            color="purple")

"""#XGBoost regressor """

import xgboost as xgb

xgb_reg=xgb.XGBRegressor()

prams={
    'learning_rate':[0.1, 0.25],
    'n_estimators':[100, 200],
    'max_depth':[10, 20,50]
}

random_clf=GridSearchCV(xgb_reg, param_grid=prams,verbose=1, return_train_score=True,
                                n_jobs=-1, cv = 4, scoring = 'neg_root_mean_squared_error')
random_clf.fit(X_trainTrans, y_train)

random_clf.best_estimator_

resultadosXGB = pd.DataFrame(random_clf.cv_results_)
display(resultadosXGB)

fig, ax = plt.subplots(figsize=(10,20))

#random_clf.best_estimator_.get_booster().feature_names = pd.get_dummies(X_train).columns

xgb.plot_importance(random_clf.best_estimator_, height=0.5, ax=ax, grid= False)
plt.show()

pd.get_dummies(X_train).columns

X_train

X_trainTrans[0,:]

"""#Red neuronal """

from re import X

X_trainT2, X_valT2, y_trainT2, y_valT2 = train_test_split(X_trainTrans, y_train, test_size=0.2, random_state=1)

np.shape(X_trainT2)

import tensorflow 
from tensorflow.keras import layers
tensorflow.random.set_seed(1234)

from keras.layers import Dense
from keras.models import Sequential
from keras.wrappers.scikit_learn import KerasRegressor

import keras 
def build_model():
  model = Sequential([

    layers.Dense(100, activation='relu', input_shape=[6]),
    layers.Dense(100, activation='relu'),
    layers.Dense(100, activation='relu'),
    layers.Dense(1)
  ])

  model.compile(loss='mean_squared_error',
                optimizer='adam',
                metrics=['mae', 'mse'])
  return model

model = build_model()
# Gradient descent algorithm

estimator = KerasRegressor(build_fn=build_model, epochs=100, batch_size=5, verbose=0)

class PrintDot(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs):
    if epoch % 20 == 0: print('\n')
    print('*', end='')

EPOCHS = 200

history = model.fit(np.asarray(X_trainT2).astype('float32'), y_trainT2, epochs=EPOCHS, validation_data = (np.asarray(X_valT2).astype('float32'),y_valT2),verbose=0,callbacks=[PrintDot()])

def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Abs Error [MPG]')
  plt.plot(hist['epoch'], hist['mae'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mae'],
           label = 'Val Error')
  plt.legend()

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Square Error [$MPG^2$]')
  plt.plot(hist['epoch'], hist['mse'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mse'],
           label = 'Val Error')
  plt.legend()
  plt.show()

plot_history(history)

import sklearn.metrics as metrics

def regression_results(y_true, y_pred):
    # Regression metrics
    explained_variance=metrics.explained_variance_score(y_true, y_pred)
    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) 
    mse=metrics.mean_squared_error(y_true, y_pred) 
    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)
    r2=metrics.r2_score(y_true, y_pred)
    print('explained_variance: ', round(explained_variance,4))    
    print('r2: ', round(r2,4))
    print('MAE: ', round(mean_absolute_error,4))
    print('MSE: ', round(mse,4))
    print('RMSE: ', round(np.sqrt(mse),4))



"""#Estadisticas de resultados"""

print('--------------------------------------------- ')
print('KNN')
print(regression_results(np.exp(y_test), np.exp(ksearch.best_estimator_.predict(X_testTrans))))

print('----------------------------------------------- ')
print('Random forest')
print(regression_results(np.exp(y_test), np.exp(gsearch.best_estimator_.predict(X_testTrans))))

print(' ----------------------------------------------')
print('XGB')
print(regression_results(np.exp(y_test), np.exp(random_clf.best_estimator_.predict(X_testTrans))))


print('----------------------------------------------')
print('Neural network')
print(regression_results(np.exp(y_test), np.exp(model.predict(X_testTrans))))

from joblib import dump, load
dump(gsearch.best_estimator_, 'random_forest.joblib') 
dump(pipe, 'pipeline.joblib')

modelo = load('random_forest.joblib') 
pipeline = load('pipeline.joblib')

# Casa o Apartamento / Latitud /Longitud / Baños /Cuartos
pred1 = modelo.predict(pipeline.transform(pd.DataFrame(data=['Casa',4.6,-74,2,3], index=['property_type','lat','lon','bathrooms','rooms_bedrooms']).T))

pd.DataFrame(data=['Casa',4.6,-74,2,3], index=['Property_type','lat','lon','bathrooms','rooms_bedrooms']).T

np.exp(pred1)

plt.figure(figsize=(15,15))
plt.yscale('log')
plt.xscale('log')
plt.scatter(np.exp(y_test), np.exp(random_clf.best_estimator_.predict(X_testTrans)))
